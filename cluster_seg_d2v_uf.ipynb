{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import gensim\n",
    "import pickle\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.models.doc2vec as d2v\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = r'ufSeg_dm.model'\n",
    "model = d2v.Doc2Vec.load(f)\n",
    "segments_matrix = model.docvecs.doctag_syn0 #get 110,701 vectors as a matrix\n",
    "segments_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2) \n",
    "X = pca.fit_transform(segments_matrix)\n",
    "\n",
    "kmeans = KMeans(n_clusters=60)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X) #vectors assigned a number 1-30\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# Percentage of variance explained for each components\n",
    "print('explained variance ratio (first two components): %s'\n",
    "      % str(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c = y_kmeans) # c = color, plotting all points in X\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=5, alpha=0.5); # plotting centroids for each cluster\n",
    "\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('PCA of UF d2v segments with K-Means')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle object: list of tuples which each contain info on individual segments\n",
    "# (book.title, book.code, book.date, toks, seg_pos)\n",
    "inp = r'premodel/d2v_UFSeg.txt' \n",
    "objs = pickle.load(open(inp, 'rb'))\n",
    "objs_new = []\n",
    "\n",
    "code_list = [x[1] for x in objs]\n",
    "from collections import Counter\n",
    "seg_count_for_book = Counter(code_list) #returns a dictionary of unique code: # of segments\n",
    "\n",
    "for obj in objs:\n",
    "    lst = list(obj) # convert tuple into a list\n",
    "    objs_new.append(lst + [seg_count_for_book[obj[1]]]) # (book.title, book.code, book.date, toks, seg_pos, seg_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "all_segments = []\n",
    "\n",
    "for i in range(len(objs_new)):\n",
    "    segment = objs_new[i]\n",
    "    title = segment[0]\n",
    "    uf_id = segment[1]\n",
    "    date = segment[2]\n",
    "    text = segment[3]\n",
    "    seg_pos = segment[4]\n",
    "    rel_pos = seg_pos/segment[5] # progress in book\n",
    "    cluster = y_kmeans[i] # i-th segment correlates to i-th cluster assignment\n",
    "    all_segments.append((uf_id, cluster, seg_pos, rel_pos, title, date, text))\n",
    "    \n",
    "labels = ['uf_id', 'cluster', 'seg_pos', 'rel_pos', 'title', 'date', 'text']\n",
    "\n",
    "df = pd.DataFrame.from_records(all_segments, columns = labels)\n",
    "small_df = df[['cluster', 'text','rel_pos']]\n",
    "#small_df.sample(20)\n",
    "small_df[small_df['cluster'] == 0].sample(30)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[df['cluster'] == 3]['rel_pos'].mean()\n",
    "#df.loc[df['cluster'] == 3]['rel_pos'].median()\n",
    "#df.groupby('cluster').mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = 'uf_kmeans_clusters_thru_book/'\n",
    "\n",
    "# plot density of all 30 clusters vs relative position in book (aggregate)\n",
    "# track peaks and troughs of each cluster\n",
    "\n",
    "for x in range(60):\n",
    "    y = df.loc[df['cluster'] == x]['rel_pos']\n",
    "    plt.figure()\n",
    "    plt.xlim(0,1)\n",
    "    plt.xlabel('relative position in book (aggregate)')\n",
    "    plt.title(x)\n",
    "    #y.plot.hist(bins = 100)\n",
    "    y.plot.kde()\n",
    "    #plt.savefig(outpath +'cluster_{}_across_book'.format(x))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add decade column\n",
    "def getDecade(year):\n",
    "    return year[:-1] + '0'\n",
    "df['decade'] = df['date'].astype('category')\n",
    "df['decade'] = df['decade'].apply(getDecade)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = 'uf_kmeans_clusters_thru_time/'\n",
    "\n",
    "# plot histogram of clusters across publication dates\n",
    "\n",
    "datetime = pd.DatetimeIndex([str(x) for x in df.date]) #convert year to datetime objects\n",
    "df['date'] = datetime\n",
    "\n",
    "for x in range(29,34):\n",
    "    y = df.loc[df['cluster'] == x]['date']\n",
    "    plt.figure()\n",
    "\n",
    "    plt.title('Frequency of cluster ' + str(x) +' across time')\n",
    "    y.hist(bins = 20)\n",
    "    #plt.savefig(outpath +'cluster_{}_across_time'.format(x))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'uf_kmeans_clusters_thru_time_segments/'\n",
    "def saveSegmentDataToCSV(outfolder):\n",
    "    for i in range(30):\n",
    "        outf = 'cluster_' + str(i) + '_dataframe.csv'\n",
    "        cluster_frame = df[df['cluster'] == i]\n",
    "        cluster_frame.to_csv(outfolder + outf)\n",
    "#saveSegmentDataToCSV(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['cluster'] == 60]['text'].sample(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from operator import itemgetter\n",
    "cluster_texts = []\n",
    "for i in range(60):\n",
    "    cluster_text = ''\n",
    "    col = df[df['cluster'] == i]['text']\n",
    "    for row in col:\n",
    "        cluster_text += ' '.join(row) + ' '\n",
    "    cluster_texts.append(cluster_text)\n",
    "\n",
    "tf = TfidfVectorizer(smooth_idf=False, norm=None, analyzer='word', max_df=0.95, min_df=.1)\n",
    "txt_fitted = tf.fit(cluster_texts)\n",
    "tfidf_matrix = txt_fitted.transform(cluster_texts)\n",
    "\n",
    "feature_names = tf.get_feature_names()\n",
    "\n",
    "doc = 4\n",
    "feature_index = tfidf_matrix[doc,:].nonzero()[1]\n",
    "tfidf_scores = zip(feature_index, [tfidf_matrix[doc, x] for x in feature_index])\n",
    "\n",
    "for w, s in [(feature_names[i], s) for (i, s) in sorted(tfidf_scores, key = itemgetter(1), reverse =True)]:\n",
    "  print(w,s)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x in unique_codes:\n",
    "    #df[df['hathi_id'] == x]\n",
    "book_props = df.groupby(['hathi_id', 'cluster']).agg({'rel_pos': 'sum'})\n",
    "# Change: groupby state_office and divide by sum\n",
    "x = book_props.groupby('hathi_id').apply(lambda x:100 * x / float(x.sum()))\n",
    "for i in unique_codes:\n",
    "    y = df[df['hathi_id'] == i]['cluster']\n",
    "    y.plot.pie()\n",
    "\n",
    "#df.groupby(['hathi_id','cluster']).agg('count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = df.groupby(['hathi_id', 'cluster']).size()\n",
    "gr.groupby(level=0).sum()\n",
    "gr = gr / gr.groupby(level=0).sum()\n",
    "gr['hathi_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
